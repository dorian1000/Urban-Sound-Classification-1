\section*{Appendix: Test score}
You may have noticed that we consequently have tested a validation set with known targets in the results. How about the test set mentioned in section \ref{sec:theory}?

The test set does not come with targets, so to check whether or not the predicted categories are correct, we need to upload the results to the contest web page. This is a big part of the contest, but in a scientific report we found it better to split the set with known targets into a training set and validation set. The test set score is therefore moved to an appendix, and is probably most interesting for people who care about the contest.

When predicting the categories in the test set, we can use all the samplings with known targets as our training set, which hopefully makes the prediction better. We have seen that the Feed-forward Neural Network (FNN) works best, so we will stick to it when predicting the test classes. The program that predicts classes and fills out the \textit{test.csv} file is found in \textit{test\_accuracy.py}. Damaged files will be denoted as \textit{'damaged'}.

Using 4 hidden layers of 1024 nodes each, ADAM optimizer with a batch size of 32 and 1000 epochs, ReLU activation function on hidden layers and softmax on the output, a dropout of 50\% in all layers and the MFCCs as inputs with 40 frames, we obtain a training accuracy of \textbf{98\%}. According to Vidhya Analytics, the test accuracy is just \textbf{60\%}, which is poor compared to the validation accuracy. Even though the test data set is way larger than the validation set (3298 against 1746), there is no reason why the accuracy should be lower if the average prediction difficulties are the same, which they should be. Maybe TensorFlow calculates the validation accuracy in another way than Vidhya Analytics? Or maybe there are issues with the CSV-file that we uploaded?