\section{Introduction} \label{sec:introduction}
In the everyday life we are continuously surrounded by sounds, and usually the human brain is able to recognize what kind of sound it is based on experience. Artificial neural networks are again based on studies of the human brain, and if the artificial neurons work as the biological ones, there should be possible to train a neural network recognizing sounds as well. 

Lately, immense efforts have been put into this subject with the purpose of translating voice into text, leaded by technology companies like Google, Microsoft, Amazon and so on, who develop voice controlled virtual assistants. The technology is promising, the time saving using voice commands vs. keyboard commands is potentially huge and the market is enormous. Some even claim that virtual assistants will run our lives within 20 years. \cite{Feloni} Whether or not this is right is still uncertain, but what is certain is that the technology has great potential.

In this final project we will, based on \href{https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/}{the Urban Sound Challenge}, sort sounds into classes using various classification methods. We use the same idea as when the great technology companies recognize voice, but we are going to differentiate between sounds made by \textbf{air conditioners}, \textbf{car horns}, \textbf{children}, \textbf{dogs}, \textbf{drills}, \textbf{engines}, \textbf{guns}, \textbf{jackhammers}, \textbf{sirens} and \textbf{street musicians}. 

After some more information about the challenge, we see how we can analyze the samplings in the time domain, frequency domain and how we can extract features. In the end of the \textit{Sound Analysis} section, we also take a quick look at some of the samplings such that we get an intuition of what they may look like.

In the \textit{Classification Methods} section, the different methods are presented including Logistic Regression, Feed-forward Neural Networks, Convolutional Neural Networks and Recurrent Neural Networks. They will all be investigated in combination with feature extraction. We will mainly stick to the ADAM optimizer, but present additionally the Gradient Descent method in the \textit{Optimization} section. 

We consider the activation function as an important tool in neural networks with plenty of options, and have therefore written a dedicated section about them, \textit{Activation}. It takes the Logistic, ReLU, Leaky ReLU and ELU functions. 

The code is described in its respective section, presenting packages used for different purposes. Also a description of the code structure and an example implementation is added. 

All the accuracy scores are presented in \textit{Results} and discussed in \textit{Discussion}. Finally, a brief conclusion is written in \textit{Conclusion}.

